{
  "name": "local-llm-examples",
  "version": "1.0.0",
  "type": "module",
  "main": "embeddings-node-llama-cpp.ts",
  "scripts": {
    "start": "npx bun run embeddings-node-llama-cpp.ts",
    "embeddings-node-llama-cpp": "npx bun run embeddings-node-llama-cpp.ts",
    "local-qa-chain": "npx bun run local-qa-chain.ts",
    "local-chain": "npx bun run local-chain.ts",
    "local-rag-chain": "npx bun run local-rag-chain.ts",
    "langchain-chat-ollama-cpp": "npx bun run langchain-chat-ollama-cpp.ts",
    "analyze-docs-ollama": "npx bun run analyze-docs-ollama.ts",
    "analyze-docs-llama-cpp": "npx bun run analyze-docs-llama-cpp.ts",
    "inspect-gpu": "npx --no node-llama-cpp inspect gpu",
    "postinstall": "npm run models:pull",
    "models:pull": "node-llama-cpp pull --dir ./models hf:mradermacher/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"
  },
  "author": "",
  "license": "ISC",
  "description": "",
  "dependencies": {
    "@langchain/community": "^0.3.16",
    "@langchain/core": "^0.3.19",
    "@langchain/langgraph": "^0.2.3",
    "@langchain/ollama": "^0.1.2",
    "@langchain/openai": "^0.3.14",
    "@langchain/textsplitters": "^0.1.0",
    "@node-llama-cpp/mac-arm64-metal": "^3.2.0",
    "@xenova/transformers": "^2.17.2",
    "cheerio": "^1.0.0",
    "faiss-node": "^0.5.1",
    "hnswlib-node": "^3.0.0",
    "langchain": "^0.3.6",
    "node-llama-cpp": "^3.2.0",
    "sharp": "^0.33.5"
  },
  "trustedDependencies": [
    "hnswlib-node",
    "node-llama-cpp",
    "protobufjs",
    "sharp"
  ]
}
